{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AULA 9 - ESTAT√çSTICA BAYESIANA\n",
    "---\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidade Condicional \n",
    "\n",
    "\n",
    "C√°lculo da probabilidade condicional de $P(C|X)$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Considere-se o problema seguinte:\n",
    "\n",
    "<img src=\"images/P_AaposB.png\" style=\"width:40%\"/>\n",
    "\n",
    "- tr√™s baldes de cor diferente $C \\in \\{Vermelho, Azul, Preto\\}$  \n",
    "- bolas com duas cores $ X \\in \\{Laranja, Verde\\}$ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Considere-se que $C$ e $X$ s√£o vari√°veis aleat√≥rias e $P(Laranja \\space|\\space Azul)$ corresponde a \n",
    "\n",
    "$$P(X = Laranja \\space|\\space C = Azul)$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "O que quer dizer probabilidade condicionada? Podemos Traduzir $P(X = Laranja \\space|\\space C = Azul)$ como \n",
    "\n",
    "> A probabilidade de encontrarmos uma bola $Laranja$, sabendo que estamos √† procura no balde $Azul$ **(Probabilidade bola Laranja, dado balde Azul)**. Neste caso a probabilidade √© dada por 1/4 (1 bola $Laranja$ num universo de 4 bolas).\n",
    "\n",
    "\n",
    "Repare-se que isto √© diferente de dizer $P(X = Laranja , C = Azul)$ que quer dizer:\n",
    "\n",
    "> A probabilidade de encontrarmos uma bola $Laranja$ e um balde azul $Azul$ **(Probabilidade bola Laranja e balde Azul)**. Neste caso a probabilidade √© dada por 1/14 (1 bola $Laranja$ num universo de 14 bolas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com esta L√≥gica, podemos preencher uma matriz (A) com todas as $P(C, X)$:\n",
    "\n",
    "|- |Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "\n",
    "Repare-se que se quisermos saber $P(C)$ (a probabiliade de retirarmos bolas de um dado balde), podemos somar as probabilidades da coluna dessa cor \n",
    "\n",
    "> $e.g.$,  Azul = 4/14, que pode ser retirada igualmente da imagem\n",
    "\n",
    "Esta regra escrita de forma formal significa que\n",
    "\n",
    "$$P(C) = \\sum_X P(C,X)$$\n",
    "\n",
    "Esta probabilidade chama-se de **probabilidade marginal** e trata-se da probabilidade de uma dada caracter√≠stica independentemente das outras.\n",
    "\n",
    "---\n",
    "\n",
    "Se a $P(X, C) = P(X)P(C)$, $X$ e $C$ dizem-se independentes. \n",
    "\n",
    "Neste caso dos baldes e das bolas n√£o s√£o: Seriam independentes se cada balde contivesse a mesma fra√ß√£o de bolas Laranjas e Verdes. Nesse caso $P(X|C) = P (X)$, de modo que a probabilidade de selecionar, digamos, uma bola verde era independente do balde escolhido e vice verse. \n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Probabilidade Condicionada\n",
    "----\n",
    "\n",
    "> Podem pensar na probabilidade condicionada como uma redu√ß√£o do universo em an√°lise ao subconjunto condicionante. Quer isto dizer, que ao criarmos uma probabilidade condicionada estamos na verdade a limitar o n√∫mero de bolas em an√°lise.\n",
    "\n",
    "\n",
    "Neste caso podemos ter duas:\n",
    "\n",
    "- $P(X \\space| \\space C)$ - a probabilidade de obtermos uma dada cor da bola conhecendo o balde. \n",
    "\n",
    "\n",
    "- $P(C\\space| \\space X)$ - a probabilidade de obtermos um dado balde conhecendo a cor da bola. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Tomemos a seguinte probabilidade: $P(Vermelho | Verde)$ que nos √© dada pelas bolas do balde vermelho sabendo sabendo que estamos a analisar somente o subconjunto (condicionante) das bolas Verdes (que tem 7 bolas no total). O resultado √© \n",
    "\n",
    "$$P(Vermelho|Verde) = 2/7$$\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**E se quisermos obter o valor da probabilidade condicionada com base nas restante probabilidades definidas anteriormente, ou seja, com base nas P(C,X), P(C) e P(X)?**\n",
    "\n",
    "\n",
    "\n",
    "|-|Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "> Considere-se para j√° este exerc√≠cio: Olhando agora para a nossa tabela, podemos obter o valor 2/7 usando as probabilidades que definimos na tabela tipo $P(X,C)$ e as probabilidades marginais $P(X)$ ou $P(C)$?\n",
    "\n",
    "- podemos dizer que o 2 vem de $P(Vermelho,Verde) =2/14$ \n",
    "- e que o 7 vem da probabilidade de estarmos a tirar bolas Verdes, $P(Verde) = 7/14$. \n",
    "\n",
    "Assim, √© facil de ver que \n",
    "\n",
    "$$P(Vermelho|Verde) = \\frac{P(Vermelho,Verde)}{P(Verde)}$$\n",
    "\n",
    "ou seja, de formalmente temos que \n",
    "\n",
    "$$P(X|C) = \\frac{P(X,C)}{P(C)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema de Bayes**\n",
    "\n",
    "\n",
    "Juntando a regra do produto com a propriedade $P(X,C) = P(C,X)$, obtemos:\n",
    "\n",
    "$$P(X,C) = P(X\\space|\\space C)P(C) <=> P(C,X) = P(C\\space|\\space X)P(X)$$\n",
    "\n",
    "ou seja, \n",
    "\n",
    "$$P(C\\space|\\space X)P(X) = P(X\\space|\\space C)P(C)$$\n",
    "\n",
    "e por isso (Teorema de Bayes):\n",
    "\n",
    "$$P(C\\space|\\space X) = \\frac{P(X\\space|\\space C)P(C)}{P(X)}$$\n",
    "\n",
    "Estas $P(X\\space|\\space C)$ e o $P(C)$ s√£o retirados dos dados.\n",
    "\n",
    "<img src=\"images/bayesian_theorem.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onde nos leva o teorema de Bayes, no que diz respeito √† aprendizagem automatica?\n",
    "\n",
    "---\n",
    "\n",
    "### Um problema Pr√°tico\n",
    "\n",
    "\n",
    "Vamos imaginar um cen√°rio em que eu quero estimar a probabilidade do balde de origem de uma bola que tenho na minha m√£o (**problema de classifica√ß√£o em que a classe √© o balde de origem e as features s√£o as cores das bolas**) mas n√£o sei a matriz A de frequ√™ncias por balde e por cor porque **n√£o tenho acesso aos dados todos ou porque s√£o muitos (imaginem milhares de cores e milhares de baldes...)**\n",
    "\n",
    "> Por exemplo, queremos conhecer a probabilidade do balde ser Azul, sabendo que a cor da bola na minha m√£o √© $Verde$ - $P(Azul \\space | \\space Verde)$?\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "Como resolver o problema? \n",
    "\n",
    "**Posso fazer algumas experi√™ncias** ou seja, tirar algumas bolas de diversos baldes e medir os resultados:\n",
    " \n",
    "1) Contar a frequ√™ncia marginal do n√∫mero bolas origin√°rias de cada balde $P(Balde)$\n",
    "\n",
    "2) Da mesma forma, tamb√©m posso contar a frequ√™ncia marginal das cores das bolas que foram saindo ao longo da experi√™ncia $P(Bola)$\n",
    "\n",
    "3) Ver a probabilidade de sair uma cor de um dado balde. Isto quer dizer que conseguimos estimar $P(Bola \\space | \\space Balde)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**mas como podem estas estat√≠sticas ajudar-nos a estimar** $P(Azul \\space | \\space Verde)$?\n",
    "\n",
    "\n",
    "Voltemos ao nosso modelo inicial:\n",
    "\n",
    "S√≥ para percebermos o valor a que quer√≠amos chegar, usando a matriz de frequencias A:\n",
    "\n",
    "|-|Vermelho|  Azul  |Preto|\n",
    "|--|:-------------:|:-------------:|:-----:|\n",
    "|**Laranja**|6/14|1/14|0/14|\n",
    "|**Verde**|2/14|3/14|2/14|\n",
    "\n",
    "\n",
    "$$P(Azul \\space | \\space Verde) = 3/7$$ \n",
    "\n",
    "\n",
    "$$P(Verde \\space | \\space Azul) = 3/4$$ (que neste caso √© conhecido perfeitamente)\n",
    "\n",
    "\n",
    "(que vem de $P(X|Y) = \\frac{P(X,Y)}{P(Y)}$ ou se percebe por an√°lise da matriz A)\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Portanto, queremos deduzir o valor de 3/7 a partir do conhecimento de 3/4: $$\\frac{3}{4} => \\frac{3}{7}$$ \n",
    "\n",
    "3 √© o factor comum (quantidade de bolas) e 4 e 7 s√£o o universo no qual estamos a definir cada uma das probabilidades, e √© isso que queremos trocar.\n",
    "\n",
    "Como $P(Azul) = \\frac{4}{14}$ e $P(Verde) = \\frac{7}{14}$\n",
    "\n",
    "\n",
    "ao fazermos \n",
    "\n",
    "$$\\frac{P(Verde \\space | \\space Azul)P(Azul)}{P(Verde)} = \\frac{3}{7}$$ \n",
    "\n",
    "estamos a retirar a nossa base de exist√™ncias do universo balde azul para o universo balde verde.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modelos de Classifica√ß√£o Bayesianos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Consideremos dois cen√°rios:\n",
    "\n",
    "1) **Prior Estimate** - N√£o temos mais informa√ß√£o sen√£o o n√∫mero de bolas total existentes em cada balde (sem saber cores);\n",
    "\n",
    "\n",
    "Neste primeiro cenario a melhor estimativa que temos √© a chamada estimativa √† priori (**Prior Estimate**):\n",
    "\n",
    "$$Max(P(Balde_j))$$\n",
    "\n",
    "\n",
    "\n",
    "> **Prior Estimate** - S√≥ temos dados dados antes de tirarmos a bola, n√£o temos dados ap√≥s o evento de retirar a bola √† posteriori, por isso se chama √† priori. \n",
    "\n",
    "---\n",
    "\n",
    "2)  **Posterior Estimate** - sabemos tamb√©m a cor da bola retirada e **s√£o conhecidas as quantidades de cada cor das bolas em cada balde**\n",
    "\n",
    "> ou seja, consigo preencher uma matriz A com experi√™ncias e usar a lei de Bayes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos genericamente, para sabermos qual a origem mais provavel de uma bola, basta encontrar a probabilidade condicional maior:\n",
    "\n",
    "\n",
    "$$\\frac{P(Azul,Verde)}{P(Verde)} = \\frac{3/14}{7/14} = 3/7$$\n",
    "\n",
    "do balde Vermelho e Preto s√£o:\n",
    "\n",
    "$$P(Vermelho| Verde)= 2/7 $$\n",
    "\n",
    "$$P(Preto| Verde)= 2/7 $$\n",
    "\n",
    "Portanto, a origem mais prov√°vel √© o balde Azul, porque maximiza a probabilidade, tendo em conta os dados √† priori (quantidades de bolas de cada cor em cada balde) e os dados √† posteriori (cor da bola, ou seja, as features do novo registo a classificar).\n",
    "\n",
    "\n",
    "E estimativa que us√°mos chama-se **Maximum Posterior Estimate (MAP)** e permite maximizar a probabilidade tendo em conta os dados (features) do exemplar retirado.\n",
    "\n",
    "A **Maximum Posterior Estimate (MAP)** √© dada por\n",
    "\n",
    "$$C_i: Max\\{P(C_i|X)\\}$$\n",
    "\n",
    "\n",
    "para $i =\\{1,...,N\\}$ onde $N$ √© o n√∫mero de baldes\n",
    "\n",
    "----\n",
    "\n",
    "O resultado do **MAP √© considerado √≥ptimo**, quando existem dados suficientes para o seu c√°lculo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na Verdade...\n",
    "\n",
    "Como vimos, Nem sempre existem dados suficientes que suportem o c√°lculo do MAP **porque a matriz de frequ√™ncias A n√£o √© representativa da estat√≠stica**: por exemplo, ou porque n√£o tenho acesso a esses dados todos ou porque s√£o muitos mais uma vez, imaginem milhares de cores e milhares de baldes...)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo o que demos at√© aqui, √© poss√≠vel ver duas abordagens para definir a fun√ß√£o $P(C_i|X)$ usada no MAP. E estas abordagens levam a m√©todos distintos, que defininem 2 grandes classes de classificadores:\n",
    "\n",
    "- **Algoritmos Discriminativos (Discriminative Algorithms)** - Que tentam aprender directamente o que √© $P(C_i|X)$ e da√≠ classificar novos registos. Exemplos s√£o a regress√£o log√≠stica (que veremos na pr√≥xima aula) as √°rvores de decis√£o e as Random Forrests. Este modelo cria fronteiras de decis√£o entre as classes. Depois, em fase de classifica√ß√£o, o modelo verifica se o nosso modelo est√° de um lado ou de outro da fronteira de decis√£o e classifica de acordo com isso.\n",
    "\n",
    "\n",
    "- **Algoritmos Generativos (Generative Algorithms)** - Que tentam aprender atrav√©s do teorema de Bayes, ou seja, dizendo que\n",
    "\n",
    "$$P(C\\space|\\space X) = \\frac{P(X\\space|\\space C)P(C)}{P(X)}$$\n",
    "\n",
    "Na fase de aprendizagem este tipo de modelos (probabilistico) criam uma defini√ß√£o sobre o que √© cada classe $C_i$. Esta defini√ß√£o √© dada por $P(X|C_i)P(C_i)$ \n",
    "\n",
    "> ou seja, quais as probabilidades de existir cada feature, dada uma classe.\n",
    "\n",
    "Na fase de classifica√ß√£o, ve qual o modelo que tem melhor score;\n",
    "O Naive Bayes que vamos estudar  √© um m√©todo deste estilo. Vamos usar o MAP, aplicado √† igualdade de Bayes, ou seja,\n",
    "\n",
    "$$C_i: Max\\{\\frac{P(X\\space|\\space C_i)P(C_i)}{P(X)}\\}$$\n",
    "\n",
    "Mas repare-se que a probabilidade de X n√£o muda entre as diversas express√µes, pelo que o m√°ximo com o denominador √© igual ao m√°ximo sem ele:\n",
    "\n",
    "$$C_i: Max\\{P(X\\space|\\space C_i)P(C_i)\\}$$\n",
    "\n",
    "> E se tent√°ssemos apenas maximizar $$C_i: Max\\{P(X\\space|\\space C_i)\\}$$ ent√£o estar√≠amos a aplicar o algoritmo maximum likelihood estimation (MLE) que √© um caso especial do MAP, quando $P(C_i)$ √© constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Tomemos ent√£o este exemplo do costume:\n",
    "\n",
    "<img src=\"images/class_2.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a express√£o anterior, ap√≥s treinadas as probabilidades, ver para um dado registo de teste X, qual a classe que maximiza $P(X\\space|\\space C_i)P(C_i)$ (isto √©, qual a probilidade posterior $P(C_i|X)$ que fica maximizada)\n",
    "\n",
    "Em que neste caso  $X$ √© definido por ```{Home Owner, Marital Status,Annual Income}```, mas vamos chamar aos atributos  com os valores $ho,ms,ai$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, √© necess√°rio perceber qual das duas probabilidades posteriores √© maior:\n",
    "\n",
    "\n",
    "- $P(ho,ms,ai \\space|\\space Yes) P(Yes)$\n",
    "- $P(ho,ms,ai \\space | \\space No )P(No)$\n",
    "\n",
    "---\n",
    "\n",
    "### Independencia condicional\n",
    "\n",
    "Se $P(X|Y,Z) = P(X|Y)P(X|Z)$, ou seja, que a rela√ß√£o condicional de Y e Z tem com X n√£o s√£o afectadas entre si. \n",
    "\n",
    "> Quer isto dizer no nosso exemplo que o estado civil, por exemplo, de nada altera a forma como o atributo \"Home Owner\" influencia a probabilidade de default e vice versa. Ou seja, um home owner influencia a probabilidade de default, da mesma forma para casados, solteiros, etc.\n",
    "\n",
    "Apesar de se tratar de uma assun√ß√£o bastante forte, permite encontrar resultados muito bons em v√°rios problemas.\n",
    "Considerar esta assum√ß√£o de indepen√™ncia √© o que d√° o nome de \"Naive\" a este modelo.\n",
    "\n",
    "---\n",
    "\n",
    "Neste caso a nossa express√£o inicial pode ser escrita na forma:\n",
    "\n",
    "$$P(ho, ms, ai \\space|\\space C_i) = P(ho \\space|\\space C_i) P(ms \\space|\\space C_i) P(ai \\space|\\space C_i)$$\n",
    "\n",
    "\n",
    "ou seja, \n",
    "\n",
    "$$C_i: Max\\{ P(C_i)\\prod{P(x_j \\space|\\space C_i)}\\}$$\n",
    "\n",
    "\n",
    "**Como estimar as probabilidades $P(X \\space|\\space C)$ e $P(C)$ a partir dos dados?**\n",
    "\n",
    "Usar as seguintes frequ√™ncias relativas:\n",
    "\n",
    "\n",
    "$$P(C_i) = N_i/N$$\n",
    "\n",
    "$$P(x_j \\space|\\space C_i) = A_{ij}/ N_c$$\n",
    "\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $N_c$ √© o n√∫mero de registos de uma dada classe na amostra de treino;\n",
    "- $N$ √© o n√∫mero de registos na amostra de treino;\n",
    "- $A_{ij}$ √© o n√∫mero de registos da classe i que tem a propriedade j;\n",
    "\n",
    "---\n",
    "\n",
    "No caso de atributos cont√≠nuo existiriam 3 poss√≠veis abordagens para o c√°lculo de $P(x_j| C_j)$:\n",
    "\n",
    "- Criar bins por intervalos\n",
    "    - um valor ordinal por bin\n",
    "    - **Esta abordagem viola no entanto a suposi√ß√£o de independ√™ncia entre features**\n",
    "    \n",
    "    \n",
    "- Divis√£o de duas vias: ```(A < v) ou (A > v)```\n",
    "    - escolher apenas uma das duas divis√µes como novo atributo\n",
    "    - **Esta abordagem √© pouco reveladora do conteudo dos dados**\n",
    "    \n",
    "\n",
    "- **(Abordagem que vamos adoptar) Assumir atributo obedece certa distribui√ß√£o de probabilidade**\n",
    "    - Normalmente √© assumida a distribui√ß√£o normal de $P(x_j| C_j)$\n",
    "    - Assim, devemos usar estimar par√¢metros de distribui√ß√£o (m√©dia e desvio padr√£o no caso de distribui√ß√£o normal)\n",
    "    - Uma vez que a distribui√ß√£o de probabilidade √© conhecida, podemos us√°-la para estimar a probabilidade condicional $P (x_j \\space|\\space C_i)$\n",
    "    \n",
    "    \n",
    "    \n",
    "---\n",
    "##### Finalmente o Algoritmo Naive Bayes √© o seguinte:\n",
    "\n",
    "- Estimar todos os $P(C_j)$\n",
    "- Estimar todos os $P(x_j| C_j)$ para todos os $x_j$ e $C_i$\n",
    "- Dada uma nova inst√¢ncia, classificar como $C_i$ se $P(C_i) Œ† P(x_j| C_j)$ √© m√°ximo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/class_2.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√°lculo do Modelo de Naive Bayes para este caso:\n",
    "\n",
    "- **Estimar todos os $P(C_j)$:**\n",
    "\n",
    "Neste caso, $i = 2$: $C_{No} = No$ e $C_{Yes} = Yes$\n",
    "\n",
    "- $P(C_{No}) = 7/10$\n",
    "- $P(C_{Yes}) = 3/10$\n",
    "\n",
    "- **Estimar todos os $P(x_j| C_j)$ para todos os $x_j$ e $C_i$**\n",
    "\n",
    "\n",
    "- $P(Income, Class=No)$\n",
    "    - sample mean = 110K \n",
    "    - sample variance = 2975\n",
    "        \n",
    "\n",
    "- $P(Income, Class=Yes)$\n",
    "    - sample mean = 90K \n",
    "    - sample variance = 50\n",
    "\n",
    "<img src=\"images/NormalDist.png\" style=\"width:40%\"/>\n",
    "\n",
    "- $P(Married|Class=No)$ = 4/7\n",
    "- $P(Divorced|Class=No)$ = 1/7\n",
    "- $P(Single|Class=No)$ = 2/7\n",
    "\n",
    "\n",
    "- $P(Married|Class=Yes)$ = 0/3\n",
    "- $P(Divorced|Class=Yes)$ = 1/3\n",
    "- $P(Single|Class=Yes)$ = 2/3\n",
    "\n",
    "\n",
    "- $P(HomeOwner=Yes|Class=No)$=3/7\n",
    "- $P(HomeOwner=No|Class=No)$=4/7\n",
    "\n",
    "\n",
    "- $P(HomeOwner=Yes|Class=Yes)$=0/3\n",
    "- $P(HomeOwner=No|Class=Yes)$=3/3    \n",
    "\n",
    "\n",
    "- **Numa dada inst√¢ncia, classificar como $C_i$ se $P(C_i) Œ† P(x_j| C_j)$ √© m√°ximo**\n",
    "\n",
    "Considere-se a nova inst√¢ncia:\n",
    "\n",
    "```{Home Owner=No, Marital Status = Divorced, Annual Income= 120K}```\n",
    "\n",
    "- **Class=No**\n",
    "    \n",
    "    $P(C_i) Œ† P(x_j| C_j) = 7/10*3/3*1/7*0.0072 = 0.00072$\n",
    "\n",
    "\n",
    "- **Class=Yes**\n",
    "    \n",
    "ùëÉ(ùê∂ùëñ)Œ†ùëÉ(ùë•ùëó|ùê∂ùëó)=...=...\n",
    "\n",
    "\n",
    "\n",
    "A Classe desta inst√¢ncia √© por isso....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propriedades Naif Bayes  \n",
    "\n",
    "BOM \n",
    "- Robusto para pontos de ru√≠do isolados\n",
    "- Trata valores em falta ignorando esse registo durante os c√°lculos de estimativa de probabilidade\n",
    "- Robusto para atributos irrelevantes \n",
    "\n",
    "MAU\n",
    "\n",
    "- A suposi√ß√£o de independ√™ncia pode n√£o ser v√°lida para alguns atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sklearn \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_train: 0.961904761905\n",
      "score_test: 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "Iris = datasets.load_iris()\n",
    "from sklearn import datasets, tree, model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "[features_train, features_test, classes_train, classes_test] = model_selection.train_test_split(Iris.data, Iris.target, test_size=0.30)\n",
    "y_pred = gnb.fit(features_train, classes_train)\n",
    "\n",
    "score_train = gnb.score(features_train, classes_train)\n",
    "score_test = gnb.score(features_test, classes_test)\n",
    "\n",
    "print(\"score_train:\", score_train)\n",
    "print(\"score_test:\", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bayes.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPC 4\n",
    "\n",
    "- Usando sklearn correr os m√©todos Decision Tree, Random Forrest e Naive Bayes para o dataset Digits, definido em baixo;\n",
    "\n",
    "- Usar o training set para executar o treino do modelo;\n",
    "\n",
    "- Comparar o erro obtido em cada m√©todo, para o testset e para o training set e expecificar se os valores s√£o os esperados;\n",
    "\n",
    "- Para um dos algoritmos, dar exemplos do test set de inst√¢ncias mal bem classificadas (2 de cada);\n",
    "\n",
    "O DataSet de d√≠gitos pode ser encontrado assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "digits.data.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SADenv",
   "language": "python",
   "name": "sadenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
